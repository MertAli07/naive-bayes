{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lapsm(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_group(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = label_group(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = lapsm(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(X, train_labels, vocab):\n",
    "    word_counts = {\"sport\": {}, \"business\": {}, \"politics\": {}, \"entertainment\": {}, \"tech\": {}}\n",
    "    for i in range(X.shape[0]):\n",
    "        l = train_labels[i]\n",
    "        for j in range(len(vocab)):\n",
    "            if(vocab[j] in word_counts[l].keys()):\n",
    "                word_counts[l][vocab[j]] += X[i][j]\n",
    "            else:\n",
    "                word_counts[l][vocab[j]] = X[i][j]\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(actual))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will try to determine which category a news article belongs to\n",
    "among five categories (Sport, Business, Politics, Entertainment, Tech).\n",
    "We will implement a Naive Bayes classifier and verify it’s performance on English News\n",
    "Dataset. As we learned in class, Naive Bayes is a simple classification algorithm\n",
    "that makes an assumption about the conditional independence of features, but it works\n",
    "quite well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here is the number of articles for each categories:\n",
    "    Sports: 346\n",
    "    Business: 336\n",
    "    Politics: 274\n",
    "    Entertainment: 273\n",
    "    Tech: 261\n",
    "\n",
    "    Here is the three words that we choose with their statistics (frequencies):\n",
    "    Sports:\n",
    "        the: 253/346\n",
    "        to: 103/346\n",
    "        in: 58/346\n",
    "    Business:\n",
    "        the: 255/336\n",
    "        to: 101/336\n",
    "        in: 66/336\n",
    "    Politics:\n",
    "        the: 220/274\n",
    "        to: 138/274\n",
    "        of: 69/274\n",
    "    Entertainment:\n",
    "        the: 202/273\n",
    "        to: 45/273\n",
    "        and: 39/273\n",
    "    Tech:\n",
    "        the: 198/261\n",
    "        to: 127/261\n",
    "        of: 93/261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Is that feasible?\n",
    "\n",
    "    Of course not. Although these words have the highest frequencies, we cannot determine the category of the article based on just the freqıency value. We should have other parameters to do this task. In the following parts of this assignment we will yse TF-IDF for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this part, we will use bag of words (BoW) data structure to vectorize our words in the articles and then, apply Naive Bayes to them. We will use both unigram and bigram for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv(\"English Dataset.csv\", encoding='cp1254')\n",
    "data = df.to_numpy()  # convert it to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# train-test split\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(data[:,1], data[:,2], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 2:\n",
      "Accuracy of unigram:  88.9261744966443\n"
     ]
    }
   ],
   "source": [
    "# PART 2\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "print(\"PART 2:\")\n",
    "# apply naive bayes to unigram\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_sentences)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "word_counts = get_word_counts(X, train_labels, vocab)\n",
    "\n",
    "labels = [\"sport\", \"business\", \"politics\", \"entertainment\", \"tech\"]\n",
    "n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "print(\"Accuracy of unigram: \", accuracy_score(test_labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram:  22.818791946308725\n"
     ]
    }
   ],
   "source": [
    "# apply naive bayes to bigram\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2,2), analyzer='word')\n",
    "X2 = vectorizer2.fit_transform(train_sentences)\n",
    "vocab2 = vectorizer2.get_feature_names_out()\n",
    "X2 = X2.toarray()\n",
    "word_counts2 = get_word_counts(X2, train_labels, vocab2)\n",
    "\n",
    "pred = predict(n_label_items, vocab2, word_counts2, log_label_priors, labels, test_sentences)\n",
    "print(\"Accuracy of bigram: \", accuracy_score(test_labels,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the accuracies abnove, the unigram gave us a better result (approx. 90%). In this implementation, the bigram gave us a bad result (around 25%) and takes too much time to execute (nearly 10 mins). When we run our algorithm, the countVectorizer takes all of the words in the article and forms a dictionary to return their frequencies, then applies naive bayes on them. We have nearly 20000 unique words. On the other hand, the bigram has nearly 220000 words. When the bigram forms its dictionary, it takes every combination of two sequential words, including the stop words. This is the reason why the bigram takes too long and gives poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to list 10 words whose presence strongly predicts that an article belongs to specific category for each five categories and another 10 words whose absence strongly predicts the same categories. Later on, we will compare the performance of using or not using the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "pountney   0.618079\n",
      "soderling  0.596906\n",
      "glazer     0.593229\n",
      "mido       0.588578\n",
      "students   0.566474\n",
      "ivanovic   0.565251\n",
      "mirza      0.561022\n",
      "conte      0.557994\n",
      "harriers   0.551360\n",
      "solskjaer  0.549857\n",
      "                0\n",
      "fiat     0.707974\n",
      "nestle   0.687244\n",
      "absa     0.672968\n",
      "ssl      0.651372\n",
      "qantas   0.645823\n",
      "mci      0.645604\n",
      "metlife  0.641346\n",
      "turkey   0.640364\n",
      "gm       0.635750\n",
      "women    0.635571\n",
      "                  0\n",
      "fraud      0.642757\n",
      "hague      0.605789\n",
      "turkey     0.582320\n",
      "the        0.570803\n",
      "hunting    0.553382\n",
      "blackpool  0.544165\n",
      "sainsbury  0.532860\n",
      "roma       0.531554\n",
      "arrested   0.517370\n",
      "casinos    0.514653\n",
      "                  0\n",
      "godzilla   0.702534\n",
      "edwards    0.623879\n",
      "christmas  0.621976\n",
      "sky        0.605766\n",
      "aguilera   0.583050\n",
      "ice        0.580039\n",
      "clark      0.571119\n",
      "duran      0.569054\n",
      "wal        0.562631\n",
      "elvis      0.552088\n",
      "                  0\n",
      "commodore  0.692548\n",
      "p2p        0.688422\n",
      "uwb        0.608684\n",
      "bt         0.599916\n",
      "cabir      0.572749\n",
      "library    0.565201\n",
      "skype      0.565115\n",
      "desktop    0.560547\n",
      "linux      0.555753\n",
      "fbi        0.551993\n"
     ]
    }
   ],
   "source": [
    "# PART 3\n",
    "# a\n",
    "category = {\"sport\": [], \"business\": [], \"politics\": [], \"entertainment\": [], \"tech\": []}\n",
    "for i in range(len(train_labels)):\n",
    "    category[train_labels[i]].append(train_sentences[i])\n",
    "\n",
    "train_narrowed = []\n",
    "labels_narrowed = []\n",
    "for cat in category.keys():\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "    tfidf_wm = tfidfvectorizer.fit_transform(category[cat])\n",
    "    tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "\n",
    "    matrix = tfidf_wm.toarray()\n",
    "    all_texts = []\n",
    "    all_words = []\n",
    "    for i in matrix:\n",
    "        max_ind = np.argpartition(i, -20)[-20:]\n",
    "        all_texts.extend(i[max_ind])\n",
    "        all_words.extend(tfidf_tokens[max_ind])\n",
    "\n",
    "    ind = np.argpartition(all_texts, -20)[-20:]\n",
    "    best10 = {}\n",
    "    for i in range(len(np.array(all_words)[ind])):\n",
    "        if(np.array(all_words)[ind][i] not in best10.keys()):\n",
    "            best10[np.array(all_words)[ind][i]] = np.array(all_texts)[ind][i]\n",
    "    best10 = {k: v for k, v in sorted(best10.items(), reverse=True ,key=lambda item: item[1])}\n",
    "    best10 = {k: best10[k] for k in list(best10)[:10]}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(best10, orient=\"index\")\n",
    "    print(df)\n",
    "    \n",
    "    min_texts = []\n",
    "    min_words= []\n",
    "\n",
    "    mat = np.copy(matrix)\n",
    "    mat[mat == 0] = 999\n",
    "    for i in mat:\n",
    "        min_ind = np.argpartition(i, 20)[:20]\n",
    "        min_texts.extend(i[min_ind])\n",
    "        min_words.extend(tfidf_tokens[min_ind])\n",
    "\n",
    "    ind_min = np.argpartition(min_texts, 20)[:20]\n",
    "    bottom10 = {}\n",
    "    for i in range(len(np.array(min_words)[ind_min])):\n",
    "        if(np.array(min_words)[ind_min][i] not in bottom10.keys()):\n",
    "            bottom10[np.array(min_words)[ind_min][i]] = np.array(min_texts)[ind_min][i]\n",
    "    bottom10 = {k: v for k, v in sorted(bottom10.items(), key=lambda item: item[1])}\n",
    "    bottom10 = {k: bottom10[k] for k in list(bottom10)[:10]}\n",
    "    dfmin = pd.DataFrame.from_dict(bottom10, orient=\"index\")\n",
    "    dfmin.head(10)\n",
    "\n",
    "    train_narrowed.extend(all_words)\n",
    "    temp= (cat+\" \")*len(all_words)\n",
    "    t_list = temp.split(\" \")\n",
    "    labels_narrowed.extend(t_list[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair of the dataframes above represent the best and the worst TF-IDF values for each category.\n",
    "To give an example, the first dataframe is the best TF-IDF results for the category \"sport\", and the second one is the worst values for the same category.\n",
    "These best10 values are the strongest words for identifying the category of the article. Same goes for the worst 10 words as well. Rather than their presence, their \"absence\" are crucial for identifying the article correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we have use the strongest words for the naive bayes implementation. We have narrowed down our train data by choosing the strongest words, and then implemented our algorithm. Compering to the unigram implementation of part 2, our vocabulary has half less words. Thus, our execution time is much shorter. In case of accuracy score, this choice of words gives slightly less accuracy then choosing all words (about 6%) but considering the time efficiency, this is a reasonable trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove stop words from our vocabulary for this implementation, you will see that in both time efficiency and accuracy score, this implementation will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 3:\n",
      "Accuracy of best10:  86.91275167785236\n"
     ]
    }
   ],
   "source": [
    "print(\"PART 3:\")\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_narrowed)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "word_counts = get_word_counts(X, labels_narrowed, vocab)\n",
    "\n",
    "n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "print(\"Accuracy of best10: \", accuracy_score(test_labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of non-stop_word best10:  94.29530201342283\n"
     ]
    }
   ],
   "source": [
    "    #b\n",
    "    vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "    X = vectorizer.fit_transform(train_narrowed)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    X = X.toarray()\n",
    "    word_counts = get_word_counts(X, labels_narrowed, vocab)\n",
    "\n",
    "    n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "    pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "    print(\"Accuracy of non-stop_word best10: \", accuracy_score(test_labels,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are interpreting our model it makes sense to remove stop words from our vectorizer because these words are generally present in almost any news articles in our database and as such they do not help in determining which category an article belongs to. It might make sense to keep stop words when we are using Bag of Word(BoW) with n-grams where n>1. For example, for category \"Entertainment\" with trigram model, we might have adjacent words such \"Whack a Mole\". When these words are taken one by one they might not help much in determining our category but when they are together like this, these words gain a meaning that helps us much more in determining its category. So we should decide on whether removing or keeping stop words from our data based on how we are interpreting the data, because its results will change depending on which methods we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
